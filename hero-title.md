This research from Anthropic examines how AI systems fail as tasks become more complex and reasoning chains grow longer. The authors decompose model errors into bias (systematic misalignment) and variance (incoherence) components, finding that failures increasingly stem from incoherent, nonsensical actions rather than systematic pursuit of unintended goals. Their results suggest future AI failures may resemble industrial accidents—a "hot mess" of random errors—rather than the coherent pursuit of misaligned objectives that some AI safety scenarios predict.